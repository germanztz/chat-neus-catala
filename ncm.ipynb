{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Pipelieline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U llama_index llama_index.llms.ollama llama_index.embeddings.ollama \\\n",
    "    llama_index.vector_stores.chroma llama-index-callbacks-arize-phoenix \\\n",
    "    llama_index.embeddings.huggingface PyPDF2 transformers torch sentence-transformers ipywidgets llama-index-llms-dashscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama \n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "# Settings.embed_model = OllamaEmbedding(model_name=\"jinaai/jina-embeddings-v2-base-es\")\n",
    "# Settings.embed_model = OllamaEmbedding(model_name=\"qllama/bge-small-en-v1.5:f16\")\n",
    "# Settings.embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"qwen3-embedding\")\n",
    "# Settings.llm = Ollama(model=\"qwen2.5:7b-instruct\")\n",
    "# Settings.llm = Ollama(model=\"qwen3\", thinking=False, request_timeout=360.0)\n",
    "\n",
    "# Settings.llm = Ollama(model=\"deepseek-r1:8b\", thinking=False, request_timeout=360.0)\n",
    "Settings.llm = Ollama(model=\"qwen3\", thinking=False, request_timeout=360.0)\n",
    "\n",
    "\n",
    "# from llama_index.llms.dashscope import DashScope\n",
    "# import os\n",
    "# load_dotenv()\n",
    "# Settings.llm = DashScope(model=\"qwen-plus\", api_key=os.getenv(\"DASHSCOPE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.ingestion import IngestionPipeline, IngestionCache, DocstoreStrategy\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "# from llama_index.core.node_parser import TokenTextSplitter\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "from llama_index.core.schema import TransformComponent\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "import asyncio\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import glob\n",
    "import PyPDF2\n",
    "import json\n",
    "import re\n",
    "from typing import ClassVar\n",
    "from llama_index.core.extractors import TitleExtractor, QuestionsAnsweredExtractor\n",
    "import random\n",
    "\n",
    "data_path = \"./data/\"\n",
    "project_name = \"neus_catala\"\n",
    "file_prefix = re.sub('[^A-Za-z0-9]', '_', f\"{project_name}_{Settings.embed_model.model_name}\")\n",
    "default_path = f\"{data_path}{file_prefix}\"\n",
    "\n",
    "chroma_file = default_path + '_chroma.db' \n",
    "ingest_cache_file = default_path + \"_cache.json\"\n",
    "\n",
    "\n",
    "db = chromadb.PersistentClient(path=chroma_file)\n",
    "chroma_collection = db.get_or_create_collection(name=project_name)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "ingest_cache = IngestionCache()\n",
    "if os.path.isfile(ingest_cache_file):\n",
    "    ingest_cache.from_persist_path(ingest_cache_file)\n",
    "\n",
    "\n",
    "class TextCleaner(TransformComponent):\n",
    "\n",
    "    rx_nl: ClassVar  = re.compile(r'(\\n\\s*\\n)')\n",
    "    rx_hyphen: ClassVar  = re.compile(r'(-\\n)')\n",
    "    rx_notdot: ClassVar  = re.compile(r'(?<!\\.)\\n')\n",
    "    rx_num_leter: ClassVar  = re.compile(r'(\\d)([a-zA-Z])')\n",
    "\n",
    "    def __call__(self, nodes, **kwargs):\n",
    "        # nodes = list(map(lambda node: TextCleaner()(node), nodes))\n",
    "        for node in nodes:\n",
    "            text = self.rx_nl.sub('\\n', node.text)\n",
    "            text = self.rx_hyphen.sub('', text)\n",
    "            text = self.rx_notdot.sub(' ', text)\n",
    "            text = self.rx_num_leter.sub(r'\\1 \\2', text)   \n",
    "            node.set_content(text)\n",
    "        return nodes\n",
    "\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        TextCleaner(),\n",
    "        SentenceSplitter(chunk_size=512),\n",
    "        Settings.embed_model,\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    "    cache=ingest_cache,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digest Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "input_dir = data_path + \"documents\"\n",
    "already_processed_files = list(set( m['file_path'] for m in chroma_collection.get(include=['metadatas'])['metadatas']))\n",
    "\n",
    "try:\n",
    "    documents = SimpleDirectoryReader(\n",
    "        input_dir=input_dir, \n",
    "        exclude=already_processed_files\n",
    "        ).load_data()\n",
    "\n",
    "    nodes = await pipeline.arun(documents=documents, show_progress=True)\n",
    "    ingest_cache.persist(ingest_cache_file)\n",
    "    len(nodes)\n",
    "except ValueError:\n",
    "    print('No new Documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Generation of the pairs Question / answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import generate_question_context_pairs\n",
    "from llama_index.core.llama_dataset.legacy.embedding import DEFAULT_QA_GENERATE_PROMPT_TMPL\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
    "import os\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "index = VectorStoreIndex.from_vector_store(vector_store)\n",
    "\n",
    "qa_file = default_path + \"_qa.json\"\n",
    "regenerate = False\n",
    "if not os.path.isfile(qa_file) or regenerate:\n",
    "    \n",
    "    nodes = [item.node for item in index.as_retriever(similarity_top_k=1000000).retrieve(\"\")]\n",
    "    # nodes = [node for node in nodes if node.metadata.get(\"file_name\") == \"NeusCatala-llibre.pdf\"]\n",
    "    nodes = [node for node in nodes if len(node.text) > 255]\n",
    "    \n",
    "    qa_dataset = generate_question_context_pairs(\n",
    "        nodes=random.sample(nodes, min(len(nodes), 50)), \n",
    "        num_questions_per_chunk=1,\n",
    "        qa_generate_prompt_tmpl=DEFAULT_QA_GENERATE_PROMPT_TMPL+ \"Use Catalan language. do not add any introductory text or formatting. /no_think\",\n",
    "    )\n",
    "    qa_dataset.save_json(qa_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator, EmbeddingQAFinetuneDataset\n",
    "\n",
    "# Convert to retriever\n",
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "# Define metrics\n",
    "metrics = [\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"]\n",
    "\n",
    "# Create RetrieverEvaluator\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(metric_names=metrics, retriever=retriever)\n",
    "qa_dataset = EmbeddingQAFinetuneDataset.from_json(qa_file)\n",
    "\n",
    "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset, show_progress=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def display_results(name, eval_results):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    columns = {\n",
    "        \"retrievers\": [name],\n",
    "        **{k: [full_df[k].mean()] for k in metrics},\n",
    "    }\n",
    "\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "\n",
    "    return metric_df\n",
    "\n",
    "display_results(\"top-2 eval\", eval_results)\n",
    "\n",
    "\n",
    "#   retrievers\thit_rate\tmrr\tprecision\trecall\tap\t        ndcg\n",
    "# nomic-embed-text\n",
    "# 0\ttop-2 eval\t0.34\t0.242333\t0.068\t0.34\t0.242333\t0.266196\n",
    "# qwen3-embedding\n",
    "# 0\ttop-2 eval\t0.82\t0.683333\t0.164\t0.82\t0.683333\t0.717547"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nest_asyncio\n",
    "from IPython.display import display, HTML\n",
    "import nest_asyncio\n",
    "\n",
    "queries = random.sample(list(qa_dataset.queries.items()),1)\n",
    "# queries = [(\"202f20ac-f9d6-4278-a25e-41311d437c33\",qa_dataset.queries[\"202f20ac-f9d6-4278-a25e-41311d437c33\"])]\n",
    "for (sample_id, sample_query) in queries:\n",
    "    sample_expected = qa_dataset.relevant_docs[sample_id]\n",
    "\n",
    "    nest_asyncio.apply()\n",
    "    eval_result = retriever_evaluator.evaluate(sample_query, sample_expected)\n",
    "    display(HTML(f'{eval_result}'))\n",
    "\n",
    "    print('ðŸ“„ Expected Nodes:')\n",
    "    for id in eval_result.expected_ids:\n",
    "        for node in index.vector_store.get_nodes(id):\n",
    "            display(HTML(f\"{id}: {node.text} ({node.metadata.get('file_name', 'N/A')})\"))\n",
    "\n",
    "    print('eval_result',eval_result) \n",
    "    \n",
    "    print('ðŸ“„ Retrieved Nodes:')\n",
    "    for id in eval_result.retrieved_ids:\n",
    "        for node in index.vector_store.get_nodes(id):\n",
    "            display(HTML(f\"{id}: {node.text} ({node.metadata.get('file_name', 'N/A')})\"))\n",
    "\n",
    "    nest_asyncio.apply()\n",
    "    query_engine = index.as_query_engine()\n",
    "    response = query_engine.query(sample_query)\n",
    "    display(HTML(f'{response}'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
